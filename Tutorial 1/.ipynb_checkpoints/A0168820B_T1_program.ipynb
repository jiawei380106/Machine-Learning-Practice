{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BT2101 Assignment 1: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaustubh Jagtap (A0168820B), Group 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt, log\n",
    "from __future__ import division\n",
    "from collections import defaultdict\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribute Selection\n",
    "\n",
    "Functions to help us select the next attribute to split on\n",
    "1. Entropy calculation\n",
    "2. Gini Index Calculation\n",
    "3. Information Gain (based on entropy)\n",
    "4. Gini Reduction (based on gini index)\n",
    "5. Decide best feature split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate entropy with any number of sample labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(sample_labels):\n",
    "    \"\"\"Input: A vector of sample labels e.g. [A,B,A,C,C,C,A,B,B,A,C,B,A,A]. \n",
    "       This input has to be for a particular segment of the attribute we are splitting on.\n",
    "       e.g. if splitting on gender attribute, the vector will be the labels for all males. OR all females. etc   \n",
    "       Output: A number between 0 and 1, the entropy\"\"\"\n",
    "    \n",
    "    # Assert np.array\n",
    "    sample_labels = np.array(sample_labels)\n",
    "    \n",
    "    # What if sample_labels are empty\n",
    "    if sample_labels.size == 0:\n",
    "        return 0  \n",
    "    \n",
    "    # What if all the labels are the same\n",
    "    class_values = np.unique(sample_labels)\n",
    "    entropy = 0     \n",
    "    \n",
    "    for value in class_values:\n",
    "        num = len(list(filter(lambda x:x==value, sample_labels)))\n",
    "        \n",
    "        if num == 0:\n",
    "            continue\n",
    "        \n",
    "        proportion = num/sample_labels.size\n",
    "        entropy -= proportion*log(proportion,2)\n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate gini index with any number of sample labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index(sample_labels):\n",
    "    \"\"\"Input: A vector of sample labels e.g. [A,B,A,C,C,C,A,B,B,A,C,B,A,A]. \n",
    "       This input has to be for a particular segment of the attribute we are splitting on.\n",
    "       e.g. if splitting on gender attribute, the vector will be the labels for all males. OR all females. etc   \n",
    "       Output: The gini index for this split\"\"\"\n",
    "    \n",
    "    # Assert np.array\n",
    "    sample_labels = np.array(sample_labels)\n",
    "    \n",
    "    # What if sample_labels are empty\n",
    "    if sample_labels.size == 0:\n",
    "        return 0  \n",
    "    \n",
    "    # What if all the labels are the same\n",
    "    class_values = np.unique(sample_labels)# Sample labels/classes; Usually (0,1), sometimes (-1,1)\n",
    "    collective_sum = 0   ## initialize the (1 - gini_index)\n",
    "    \n",
    "    for value in class_values:\n",
    "        num = len(list(filter(lambda x:x==value, sample_labels)))\n",
    "                \n",
    "        proportion = num/sample_labels.size\n",
    "        collective_sum += proportion**2\n",
    "    \n",
    "    gini_index = 1 - collective_sum\n",
    "    \n",
    "    return gini_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain(curr_df, output, selected_attribute):\n",
    "    '''\n",
    "    This function is used to calculate the difference in the starting entropy and new entropy\n",
    "    when a given tree node is splitted by a given feature.\n",
    "    \n",
    "    Inputs:\n",
    "    1) curr_df: Samples in the current tree node before making split on the attribute (Pandas Dataframe)\n",
    "    2) output: Name of the output column\n",
    "    3) selected_attribute: Name of the feature/ attribute used to split the current tree node.\n",
    "    \n",
    "    Outputs:\n",
    "    1) measure_weighted: The weighted average of the measure. \n",
    "    2) info_gain: How much information is gained (entropy_start - entropy_weighted) if the current tree node is splitted \n",
    "    by the attribute.\n",
    "    3) subsamples (dict): All labels for the different values within selected attribute. \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # calculate overall entropy of dataframe\n",
    "    entropy_start = entropy(curr_df[output])\n",
    "    \n",
    "    # list of segments/ values that our attribute can take e.g. [male, female]\n",
    "    attribute_groups = curr_df[selected_attribute].unique()\n",
    "    \n",
    "    # initialize weighted entropy\n",
    "    entropy_weighted = 0   \n",
    "    \n",
    "    # Split samples by attribute values into subsamples\n",
    "    subsamples = defaultdict()\n",
    "    \n",
    "    for value in attribute_groups:\n",
    "        \n",
    "        # initialise dataframe pertaining to this value\n",
    "        labels_for_value = curr_df[curr_df[selected_attribute] == value]\n",
    "      \n",
    "        # Consolidate every group into subsamples. Key of dict = value (e.g. male/female). Value of dict = dataframe for that value\n",
    "        subsamples[value] = labels_for_value\n",
    "        \n",
    "        # Calculate the entropy for each value\n",
    "        entropy_for_this_value = entropy(labels_for_value[output])\n",
    "\n",
    "        entropy_weighted += len(labels_for_value)*entropy_for_this_value/len(curr_df)\n",
    "    \n",
    "    info_gain = entropy_start - entropy_weighted\n",
    "\n",
    "\n",
    "    return (entropy_weighted, info_gain, subsamples)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate reduction in gini index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_reduction(curr_df, output, selected_attribute):\n",
    "    '''\n",
    "    This function is used to calculate the difference in the starting gini index and new gini index\n",
    "    when a given tree node is splitted by a given feature.\n",
    "    \n",
    "    Inputs:\n",
    "    1) curr_df: Samples in the current tree node before making split on the attribute (Pandas Dataframe)\n",
    "    2) output: Name of the output column\n",
    "    3) selected_attribute: Name of the feature/ attribute used to split the current tree node.\n",
    "    \n",
    "    Outputs:\n",
    "    1) gini_end: The weighted average of the gini index for this attribute. \n",
    "    2) reduction: How much reduction in impurity of dataset (gini_start - gini_end) if the current tree node is splitted \n",
    "    by the attribute.\n",
    "    3) subsamples (dict): All labels for the different values within selected attribute. \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # calculate overall entropy of dataframe\n",
    "    gini_start = gini_index(curr_df[output])\n",
    "    \n",
    "    # list of segments/ values that our attribute can take e.g. [male, female]\n",
    "    attribute_groups = curr_df[selected_attribute].unique()\n",
    "    \n",
    "    # initialize gini index for this attribute\n",
    "    gini_end = 0   \n",
    "    \n",
    "    # Split samples by attribute values into subsamples\n",
    "    subsamples = defaultdict()\n",
    "    \n",
    "    for value in attribute_groups:\n",
    "        \n",
    "        # initialise dataframe pertaining to this value\n",
    "        labels_for_value = curr_df[curr_df[selected_attribute] == value]\n",
    "      \n",
    "        # Consolidate every group into subsamples. Key of dict = value (e.g. male/female). Value of dict = dataframe for that value\n",
    "        subsamples[value] = labels_for_value\n",
    "        \n",
    "        # Calculate the entropy for each value\n",
    "        gini_for_this_value = gini_index(labels_for_value[output])\n",
    "\n",
    "        gini_end += len(labels_for_value)*gini_for_this_value/len(curr_df)\n",
    "    \n",
    "    reduction = gini_start - gini_end\n",
    "\n",
    "\n",
    "    return (gini_end, reduction, subsamples)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to decide best feature split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_feature_split(curr_df, output, attributes, method):\n",
    "  \n",
    "  '''\n",
    "  This function is used to determine the best attribute to split based on maximized information gain.\n",
    "  \n",
    "  Inputs:\n",
    "  1) curr_df: Samples in the current tree node before making split on the attribute (Pandas Dataframe)\n",
    "  2) output: Name of the output column\n",
    "  3) attributes: A list of feature names\n",
    "  4) method: Method employed to evaluate the improvement in the purity of dataset \n",
    "  (method is either info_gain or gini_reduction)\n",
    "\n",
    "  Outputs:\n",
    "  1) best_attribute: The best feature which is used to do binary splitting\n",
    "  2) best_improvement: The greatest gini_reduction or information_gain (depending on method employed)\n",
    "  3) best_subsamples (dict): Data samples of best feature      \n",
    "\n",
    "  '''\n",
    "  \n",
    "  # Initialise best feature, best info_gain/reduction, best subsamples\n",
    "  best_attribute = None\n",
    "  best_improvement = 0\n",
    "  best_subsamples = None\n",
    "  \n",
    "  # Number of rows in the data samples\n",
    "  num_instances = float(len(curr_df))\n",
    "  \n",
    "  # Loop through attributes and find the best attribute to split on\n",
    "  for attribute in attributes:    \n",
    "    \n",
    "    current_split = method(curr_df, output, attribute)\n",
    "    \n",
    "    improvement = current_split[1]    # either info_gain or gini reduction\n",
    "    subsamples = current_split[2]\n",
    "    \n",
    "    # Check if attribute is better\n",
    "    if improvement >= best_improvement:\n",
    "      best_attribute, best_improvement, best_subsamples = (attribute, improvement, subsamples)\n",
    "     \n",
    "    \n",
    "  \n",
    "  return (best_attribute, best_improvement, best_subsamples) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping Conditions for pre-pruning\n",
    "\n",
    "1. The samples' labels in the current node are the same\n",
    "2. All the features have already been used for split\n",
    "3. The current tree has already reached maximum depth max_depth\n",
    "4. The number of samples in the current node is lower than minimum number min_number\n",
    "5. The information gain for the current split is lower than a threshold min_infogain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping Condition 1: The samples' labels in the current node are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_1(node_labels):\n",
    "    '''This function is used to verify whether stopping condition 1 is satisfied.\n",
    "    Inputs:\n",
    "    1) node_labels: The samples' labels in the current node\n",
    "    \n",
    "    Outputs:\n",
    "    1) True if they are all the same, False if otherwise\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # numpy array\n",
    "    node_labels = np.array(node_labels)\n",
    "    \n",
    "    # Empty labels\n",
    "    if len(node_labels) == 0:\n",
    "        return True\n",
    "    \n",
    "    if len(np.unique(node_labels)) == 1:\n",
    "        print(\"Stopping Condition 1: The samples' labels in the current node are the same\")\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping Condition 2: All the features have already been used for split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_2(features):\n",
    "    '''This function is used to verify whether stopping condition 2 is satisfied.\n",
    "    Inputs:\n",
    "    1) features: A list of feature names\n",
    "    \n",
    "    Outputs:\n",
    "    1) True if the feature list is empty, False if otherwise\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if len(features) == 0 or features == None:\n",
    "        print(\"Stopping Condition 2: All the features have already been used for split\")\n",
    "        return True\n",
    "    else:\n",
    "        return False  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping Condition 3: The current tree has already reached maximum depth max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_3(tree_depth, max_depth):\n",
    "    '''This function is used to verify whether stopping condition 3 is satisfied.\n",
    "    Inputs:\n",
    "    1) tree_depth: The depth of the current tree\n",
    "    2) max_depth: Maximum tree depth\n",
    "    \n",
    "    Outputs:\n",
    "    1) True if the current depth reaches maximum depth, False if otherwise\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if tree_depth >= max_depth:\n",
    "        print(\"Stopping Condition 3: The current tree has already reached maximum depth\")\n",
    "        return True\n",
    "    else:\n",
    "        return False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping Condition 4: Number of samples in the current node is lower than  min_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_4(samples, min_number):\n",
    "    '''This function is used to verify whether stopping condition 4 is satisfied.\n",
    "    Inputs:\n",
    "    1) samples: Data samples in the current node (Pandas DataFrame)\n",
    "    2) min_number: Minimum number of node size\n",
    "    \n",
    "    Outputs:\n",
    "    1) True if sample size is smaller than the minimum number, False if otherwise\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if samples.size <= min_number:\n",
    "        print(\"Stopping Condition 4: The number of samples in the current node is lower than minimum number\")\n",
    "        return True\n",
    "    else:\n",
    "        return False  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping Condition 5: The information gain for the current split is lower than threshold min_infogain¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info_gain(samples, output, feature) -> information gain, left, right\n",
    "# best_feature_split(samples, output, features) -> feature, information gain, left, right\n",
    "def stop_5(info_gain, min_infogain):\n",
    "    '''This function is used to verify whether stopping condition 5 is satisfied.\n",
    "    Inputs:\n",
    "    1) info_gain: Information gain after this best split\n",
    "    2) min_infogain: Minimum information gain\n",
    "    \n",
    "    Outputs:\n",
    "    1) True if information gain after this best splitting is smaller than the minimum number, False if otherwise\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if info_gain <= min_infogain:\n",
    "        print(\"Stopping Condition 5: The information gain for the current split is lower than a threshold\")\n",
    "        return True\n",
    "    else:\n",
    "        return False  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to assign predicted label based on highest count (majority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(output_labels):\n",
    "    '''\n",
    "    This function is used to get predicted label based on \"Majority Voting\" criterion for the current leaf node.     \n",
    "    Inputs:\n",
    "    1) output_labels: Outputs (labels) in this leaf node, such as [A, B, A, C, C, B]\n",
    "    \n",
    "    Outputs:\n",
    "    1) prediction: Predicted label for this leaf node (e.g. A)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # numpy array\n",
    "    output_labels = np.array(output_labels)\n",
    "    \n",
    "    # Empty label\n",
    "    if output_labels.size == 0:\n",
    "        return None\n",
    "    \n",
    "    # Count output labels (0/-1 or 1)\n",
    "    values = np.unique(output_labels)\n",
    "    \n",
    "    # Initialise a list containing a set of tuples that includes the value and len(value)\n",
    "    lst = list(map(lambda x:(x, len(output_labels[output_labels == x])), values))\n",
    "    \n",
    "    # Identify the value with the highest count\n",
    "    # Prediction based on \"Majority Voting\" criterion\n",
    "    \n",
    "    return max(lst, key=lambda x:x[1])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to generate classification tree, with any number of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClassificationTree(curr_df, output, attributes, step, tree_depth, max_depth, min_number, min_improvement, method):\n",
    "    '''This function is used to build a classification tree in a recursive way.\n",
    "       Remember how you build a binary tree in the previous C++ and Data Structure courses).\n",
    "       \n",
    "    Inputs:\n",
    "    1) curr_df: Samples in the current tree node before making split on the feature (Pandas Dataframe)\n",
    "    2) output: Name of the output column\n",
    "    3) attributes: A list of feature names\n",
    "    4) step: The current binary split step\n",
    "    5) tree_depth: The depth of the current tree\n",
    "    6) max_depth: Maximum depth this tree can grow\n",
    "    7) min_number: Minimum number of node size\n",
    "    8) min_improvement: Minimum information gain or gini reduction\n",
    "    9) method: Method to measure improvement in the purity of dataset (info_gain or gini_reduction)\n",
    "    \n",
    "    Outputs:\n",
    "    1) tree_nodes: Nested tree nodes, which are stored and shown in nested dictionary type\n",
    "    \n",
    "    Return format:\n",
    "    \n",
    "    {'best_attribute': <> ,\n",
    "     'label': <>,\n",
    "     'subtree 0': {'best_attribute': <>,\n",
    "                   'label': <>,\n",
    "                   'subtree 0': {'best_attribute': <>,\n",
    "                                 'label': '<>',\n",
    "                                 'sub_trees': None},\n",
    "                   'subtree 1': {'best_attribute': <>,\n",
    "                                 'label': <>,\n",
    "                                 'subtree 0': {'best_attribute': <>,\n",
    "                                               'label': <>,\n",
    "                                               'sub_trees': None},\n",
    "                                 'subtree 1': {'best_attribute': <>,\n",
    "                                               'label': <>,\n",
    "                                               'sub_trees': None},\n",
    "                                 'subtree 2': {'best_attribute': <>,\n",
    "                                               'label': <>,\n",
    "                                               'sub_trees': None}},\n",
    "                   'subtree 2': {'best_attribute': <>,\n",
    "                                 'label': <>,\n",
    "                                 'sub_trees': None}}\n",
    "\n",
    "    \n",
    "    '''\n",
    "    # Available attributes that we can split on\n",
    "    available_attributes = list(attributes)\n",
    "    \n",
    "    # Output labels in the current tree node\n",
    "    labels = curr_df[output]\n",
    "    \n",
    "    print (\"----------------------------------------------------------------------------\")\n",
    "    print (\"----------------------------------------------------------------------------\")\n",
    "    print (\"Step %s: Current tree depth is %s. Current tree node has %s data points and %s instances.\" \n",
    "           % (step, tree_depth, curr_df.size, curr_df.shape[0]))\n",
    "    \n",
    "    # Verify whether stopping conditions 1-4 are satisfied. If satisfied, return a leaf_node\n",
    "    if stop_1(labels) or stop_2(available_attributes) or stop_3(tree_depth, max_depth) or stop_4(curr_df, min_number):\n",
    "      return {\n",
    "          'label': majority_vote(labels),\n",
    "          'sub_trees': None, # dict\n",
    "          'best_attribute': None\n",
    "      }\n",
    "    \n",
    "    # If pass stopping conditions 1-4, then do best splitting\n",
    "    best_split = best_feature_split(curr_df, output, available_attributes, method) # outputs: best_attribute, best_improvement, best_subsamples (dict)\n",
    "    best_attribute, best_improvement, best_subsamples = best_split\n",
    "    \n",
    "    # Verify whether stopping condition 5 is satisfied. If satisfied, return a leaf node\n",
    "    if stop_5(best_improvement, min_improvement):\n",
    "      return {\n",
    "          'label': majority_vote(labels),\n",
    "          'sub_trees': None, # dict\n",
    "          'best_attribute': None\n",
    "      }\n",
    "    # If pass stopping condition 5, then move on\n",
    "    step += 1\n",
    "    \n",
    "    # print out message for logging purpose\n",
    "    message = \"Step {}: Binary split on {}. Sizes are \".format(step, best_attribute)\n",
    "      \n",
    "    for attribute_value in best_subsamples:\n",
    "      num_instances = len(best_subsamples[attribute_value])\n",
    "      message += \"[{}: {}]\".format(attribute_value, num_instances)\n",
    "    \n",
    "    print(message)\n",
    "    \n",
    "    # Remove this attribute if this attribute is used for split\n",
    "    available_attributes.remove(best_attribute)\n",
    "    \n",
    "    # Do split on tree in a recursive way\n",
    "    sub_trees = [] # initialise an empty list to store the recursive calls\n",
    "    for attribute_value in best_subsamples:\n",
    "      this_split = ClassificationTree(best_subsamples[attribute_value],\n",
    "                                      output,\n",
    "                                      available_attributes,\n",
    "                                      step+1,\n",
    "                                      tree_depth+1,\n",
    "                                      max_depth,\n",
    "                                      min_number,\n",
    "                                      min_improvement,\n",
    "                                      method\n",
    "                                     )\n",
    "      sub_trees.append(this_split)\n",
    "    \n",
    "    # Initialise final_dict to unpack the sub_trees list\n",
    "    final_dict = {'label': None,\n",
    "                  'best_attribute': best_attribute\n",
    "                 }\n",
    "    \n",
    "    for i in range(len(sub_trees)):\n",
    "      this_subtree = \"subtree {}\".format(i)\n",
    "      final_dict[this_subtree] = sub_trees[i]\n",
    "    \n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate misclassification rate before running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def misclassification_rate(actual, predicted):\n",
    "    '''\n",
    "    This function is used to assess the effectiveness of each split based on misclassification percentage.\n",
    "    Used during post-pruning.\n",
    "    \n",
    "    Inputs:\n",
    "    1) Actual: Vector of actual labels\n",
    "    2) Predicted: The predicted label assigned to that node\n",
    "    \n",
    "    Output: A percentage value, the error rate\n",
    "    '''\n",
    "    \n",
    "    misclass_count = 0\n",
    "    for label in actual:\n",
    "        if label != predicted:\n",
    "            misclass_count += 1\n",
    "            \n",
    "    return misclass_count/len(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate misclassification rate after running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def misclassification_rate(actual, predicted):\n",
    "    '''\n",
    "    This function is used to assess the effectiveness of the prediction of the model, via misclassification metric\n",
    "    Inputs:\n",
    "    1) Actual: Vector of actual labels\n",
    "    2) Predicted: Vector of predicted labels\n",
    "    \n",
    "    Output: A percentage value, the error rate\n",
    "    '''\n",
    "    misclass_count = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] != predicted[i]:\n",
    "            misclass_counter += 1\n",
    "    \n",
    "    return misclass_counter/len(actual)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 What if the functions are continuous? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the features are continuous, we will not be able to effectively use decision trees, as they are more suitable for dealing with categorical variables. Some alternatives would be to either use the nearest neighbour classification, regression methods, or neural nets (e.g. for time series). On the other hand, if we want to still use decision trees, we would need to discretize the continuous attributes. \n",
    "\n",
    "One way to do this would be to assign buckets (e.g. weights from 0-10, 10-20 etc…). This falls under global discretization, and the resultant categorical variable can be treated as a new label regardless of the choice of our data mining algorithm. However, one drawback of this is that the data loses its meaning and out predictions are the buckets instead of the actual values.\n",
    "\n",
    "Another way to still carry out decision tree classification would be to perform local discretisation, an example of which is rounding off to the nearest integer. Although this can increase the accuracy of the predictions, it introduces too many features and can lead to eventual overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 What if the output is continuous? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CART\n",
    "If the output is continuous, then one solution would be to build a regression tree. In this process, we choose a predictor and a cutpoint, and partition the space such that the loss is minimized. A simple way to measure loss is to calculate the residual sum of squares (RSS). \n",
    "\n",
    "After obtaining this predictor and cutpoint, we recursively perform binary splitting on one of the 2 spaces and stop when some pre-defined termination condition is satisfied.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
